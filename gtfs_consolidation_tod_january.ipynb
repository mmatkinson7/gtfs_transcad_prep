{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consolidating Route Patterns in GTFS\n",
    "\n",
    "Occurs after running R script that filters the trips, schedules, etc. by a chosen date.\n",
    "This part first came after an import to TransCAD but this is an alternate approach of consolidating solely based on GTFS instead of trying to join the calculated trips per Route_Name back to the route_pattern_id. There isn't a great field to join back on as we don't know exactly how TransCAD is translating the GTFS information into the Route_Names and how the routes are being consolidated as the number of unique route patterns differs between TransCAD and GTFS with GTFS having less unique route patterns in its trip table *(both post R filtering)\n",
    "\n",
    "As discussed with Marty and Sabiheh, the goal of this consolidation step is to re-assign route patterns with low numbers of trips across the day ( less than three in each time period) to route_pattern_ids with the most trips within their matching Route, Direction, and Headsign. This gets filtered down to the trip level where if a trip's route pattern is consolidated, it will be replaced with the route_pattern_id with max trips.\n",
    "\n",
    "Notes:\n",
    "- All trips with same headsign/route/direction  are assigned the same route_pattern_id. The route_pattern_id that represents the group is the route_pattern_id with the most trips per day.\n",
    "- If less than 3 trips per tod period across all day, assign the route_pattern_id of the dir/route group with the most trips.\n",
    "\n",
    "- The MBTA service day is 3AM - 26:59. For TOD assignment, everything that isn't between the hours of 6:30 - 19:00 is counted as NT. This addresses this issue as it includes 19:00-26:39 AND 3:00 - 6:30.\n",
    "\n",
    "\n",
    "Needs/Steps:\n",
    "- Number of trips per time period per route_pattern_id\n",
    "    - Midpoint time of each trip\n",
    "    - Each trip classified by TOD (based on midpoint)\n",
    "    - Sum of trips per TOD by route_pattern_id\n",
    "- route_pattern_id with most daily trips per Route, Direction, Headsign\n",
    "    - Sum all tod trips per route_pattern_id\n",
    "    - grab just the max per Route, Direction, Headsign (but keep route_pattern_id)\n",
    "- consolidate route patterns by Route, Direction, Headsign\n",
    "    - if route_pattern_id has less than 3 trips in each of the 4 TODs, replace with max trips route_pattern_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('agg')  # allows notebook to be tested in Travis\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandana as pdna\n",
    "import time\n",
    "\n",
    "import urbanaccess as ua\n",
    "from urbanaccess.config import settings\n",
    "from urbanaccess.gtfsfeeds import feeds\n",
    "from urbanaccess import gtfsfeeds\n",
    "from urbanaccess.gtfs.gtfsfeeds_dataframe import gtfsfeeds_dfs\n",
    "from urbanaccess.network import ua_network, load_network\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required bbox including all of Massachusetts and RI as well as parts of NH, CT, NY\n",
    "bbox = (-73.7207, 41.1198, -69.7876, 43.1161)\n",
    "# path to the downloaded and cleaned gtfs - mbta recap file for fall 2018\n",
    "#   this could also be a folder of gtfs folders (pre merge of multiple gtfs)\n",
    "\n",
    "path_to_gtfs = r\"J:\\Shared drives\\TMD_TSA\\Model\\networks\\Transit\\gtfs\\mbta_2019\\Part_2_GTFS_R\\mbta2018_102418_20221207\" #r\"J:\\Shared drives\\TMD_TSA\\Model\\networks\\Transit\\gtfs\\bnrd\\1_gtfs_r\"\n",
    "out_path = r\"J:\\Shared drives\\TMD_TSA\\Model\\networks\\Transit\\gtfs\\mbta_2019\\Part_3_Py_Consolidation\\mbta2018_post_py2\" #r\"J:\\Shared drives\\TMD_TSA\\Model\\networks\\Transit\\gtfs\\bnrd\\2_gtfs_py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking GTFS text file header whitespace... Reading files using encoding: utf-8 set in configuration.\n",
      "GTFS text file header whitespace check completed. Took 0.66 seconds\n",
      "--------------------------------\n",
      "Processing GTFS feed: mbta2018_102418_20221207\n",
      "The unique agency id: mbta was generated using the name of the agency in the agency.txt file.\n",
      "Unique agency id operation complete. Took 0.01 seconds\n",
      "Unique GTFS feed id operation complete. Took 0.00 seconds\n",
      "No GTFS feed stops were found to be outside the bounding box coordinates\n",
      "mbta2018_102418_20221207 GTFS feed stops: coordinates are in northwest hemisphere. Latitude = North (90); Longitude = West (-90).\n",
      "Appended route type to stops\n",
      "Appended route type to stop_times\n",
      "--------------------------------\n",
      "Added descriptive definitions to stops, routes, stop_times, and trips tables\n",
      "Successfully converted ['departure_time'] to seconds past midnight and appended new columns to stop_times. Took 1.08 seconds\n",
      "1 GTFS feed file(s) successfully read as dataframes:\n",
      "     mbta2018_102418_20221207\n",
      "     Took 2.93 seconds\n"
     ]
    }
   ],
   "source": [
    "loaded_feeds = ua.gtfs.load.gtfsfeed_to_df(gtfsfeed_path= path_to_gtfs,\n",
    "                                           validation=True,\n",
    "                                           verbose=True,\n",
    "                                           bbox=bbox,\n",
    "                                           remove_stops_outsidebbox=False,\n",
    "                                           append_definitions=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separate Out Bus Routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate out the bus routes and non-bus routes if multiple mode types\n",
    "def separate_bus(route_table,trip_table):\n",
    "    non_bus_routes = list(route_table.query('route_type != 3').route_id.unique())\n",
    "    non_bus_trips = trip_table.query('route_id in @non_bus_routes')\n",
    "    bus_trips = trip_table.query('route_id not in @non_bus_routes')\n",
    "\n",
    "    if len(non_bus_trips) == 0:\n",
    "        print(\"All routes in this GTFS are bus routes.\")\n",
    "    if len(bus_trips) == 0:\n",
    "        print(\"There are no bus routes in this GTFS.\")\n",
    "    return(non_bus_trips, bus_trips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_bus_trips, bus_trips = separate_bus(gtfsfeeds_dfs.routes, gtfsfeeds_dfs.trips)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Trips per TOD (per route_pattern_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_start_stop_times(stop_times):    \n",
    "    '''for every trip, grab the start time and stop time of the trip'''\n",
    "    chocula =0 \n",
    "    for trip_id in stop_times['trip_id'].unique():\n",
    "        max_row = stop_times.query('trip_id==@trip_id').query('stop_sequence == stop_sequence.max()')[['trip_id','arrival_time']]\n",
    "        min_row = stop_times.query('trip_id==@trip_id').query('stop_sequence == stop_sequence.min()')[['trip_id','arrival_time']]\n",
    "        r2 = min_row.merge(max_row, how='left', on='trip_id', suffixes = ('_start','_end'))\n",
    "        if chocula == 0:\n",
    "            flintstone = pd.DataFrame(r2)\n",
    "        else:\n",
    "            flintstone=pd.concat([flintstone,r2])\n",
    "        chocula +=1\n",
    "    return(flintstone)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_tod(start_stop):\n",
    "    '''calculate midpoint of trip, use midpoint to assign TOD'''\n",
    "    \n",
    "    start_stop['at_end_dec'] = (\n",
    "        (\n",
    "            (start_stop['arrival_time_end'].str.split(\":\").str[0]).astype('int32')\n",
    "            +\n",
    "            ((start_stop['arrival_time_end'].str.split(\":\").str[1]\n",
    "            ).astype('int32')/60)))\n",
    "    start_stop['at_start_dec'] = (\n",
    "        (\n",
    "            (start_stop['arrival_time_start'].str.split(\":\").str[0]).astype('int32')\n",
    "            +\n",
    "            ((start_stop['arrival_time_start'].str.split(\":\").str[1]\n",
    "            ).astype('int32')/60)))\n",
    "    \n",
    "    start_stop['midpoint'] = start_stop['at_start_dec'] + ((start_stop['at_end_dec']-start_stop['at_start_dec'])/2)\n",
    "    start_stop['tod'] = np.where(start_stop['midpoint'].between(6.50,9.50),'AM', np.where(\n",
    "        start_stop['midpoint'].between(9.50,15.00), 'MD', np.where(\n",
    "            start_stop['midpoint'].between(15.00,19.00),'PM', 'NT' \n",
    "        )\n",
    "            ) \n",
    "        )\n",
    "    \n",
    "    return start_stop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_stop = get_start_stop_times(gtfsfeeds_dfs.stop_times) # simpson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_stop_tod = assign_tod(start_stop) # smurf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine route_pattern_id with Max Number of Daily Trips\n",
    "- Getting duplicate ids where same number of trips for max, so choosing one arbitrarily.\n",
    "- The section can be expanded to try to determine which has the most trips during peak period, but for now this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR R/H/D, select RPID with most trips\n",
    "def max_daily_trips(trips):\n",
    "        # add TOD to trip table\n",
    "    tod_trips = trips.merge(start_stop_tod[['trip_id','tod']], how='left', on='trip_id')\n",
    "\n",
    "        # get the number of trips per R/H/D & rpid\n",
    "    day_rpid = tod_trips.groupby(by=['route_id','trip_headsign','direction_id','route_pattern_id']).agg({'trip_id':'nunique'})\n",
    "    day_rpid = day_rpid.rename(columns = {'trip_id':'daily_trips'}).reset_index()\n",
    "\n",
    "        # for each R/H/D, select just the route_pattern_id with the most daily trips\n",
    "    max_rpid = day_rpid.groupby(by=['route_id','trip_headsign','direction_id']).apply(lambda g: g[g['daily_trips'] == g['daily_trips'].max()])[['route_pattern_id','daily_trips']].reset_index()\n",
    "    max_rpid = max_rpid[['route_id','trip_headsign','direction_id','route_pattern_id']].rename(columns = {'route_pattern_id':'route_pattern_id_new'})\n",
    "\n",
    "    # because there are several cases where the max is held by two different route_pattern_ids, chose one arbitrarily\n",
    "    max_rpid = max_rpid.drop_duplicates(subset=['route_id','trip_headsign','direction_id'])\n",
    "    return(max_rpid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_trips_rpid = max_daily_trips(bus_trips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update rpid to rpid with max trips\n",
    "def update_rpid_max(trips, max_trips_rpid, base_trip_columns):\n",
    "    # join trip table with the RPID with most trips (join is on R/H/D, RPID is an attribute of the table)\n",
    "    trips_update_rpid = trips.merge(max_trips_rpid, how='left', on=['route_id','trip_headsign','direction_id'])\n",
    "\n",
    "    # update the route_pattern_ids based on R/H/D max\n",
    "        # this may be the same as original RPID as this should cover all R/H/D combos (therefore all trips)\n",
    "    trips_update_rpid['route_pattern_id'] = trips_update_rpid['route_pattern_id_new']\n",
    "    trips_update_rpid = trips_update_rpid[base_trip_columns]\n",
    "    \n",
    "    return(trips_update_rpid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_update1 = update_rpid_max(bus_trips, max_trips_rpid, gtfsfeeds_dfs.trips.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update route/headsign/direction combos where each TOD period has less than three trips per route_pattern_id\n",
    "- take the route_pattern_id with max number of daily trips per route/direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_RHD_under_3(trips_update_rpid, start_stop_tod):\n",
    "    # add tod back to trips table\n",
    "    tod_trips = trips_update_rpid.merge(start_stop_tod[['trip_id','tod']], how='left', on='trip_id')\n",
    "\n",
    "    # for each TOD and R/H/D count trips per TOD, get number of unique RPIDs, and unique ServiceIDs\n",
    "    tod_rpid = tod_trips.groupby(by=['route_id','trip_headsign','direction_id','tod']).agg({'tod':'count','route_pattern_id':'nunique','service_id':'nunique'})\n",
    "    tod_rpid = tod_rpid.rename(columns = {'tod':'trips_per_tod'}).reset_index()\n",
    "\n",
    "    # select just the trips that need to be updated\n",
    "    tod_rpid_4update = tod_rpid[['route_id','trip_headsign','direction_id','tod','trips_per_tod']]\n",
    "        # for R/H/D, get trips_per_tod separated into columns by TOD\n",
    "    tod_rpid_pivot = tod_rpid_4update.pivot_table(index = ['route_id','trip_headsign','direction_id'], columns = ['tod'])\n",
    "\n",
    "        # get just the trips where all 4 periods have less than 3 trips\n",
    "    needs_update = tod_rpid_pivot['trips_per_tod'].reset_index().fillna(0).query('AM < 3 & MD < 3 & PM < 3 & NT < 3')\n",
    "\n",
    "    return(needs_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_need_update = update_RHD_under_3(trips_update1, start_stop_tod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of trips per R/D & rpid\n",
    "day_rpid2 = tod_trips2.groupby(by=['route_id','direction_id','route_pattern_id']).agg({'trip_id':'nunique'})\n",
    "day_rpid2 = day_rpid2.rename(columns = {'trip_id':'daily_trips'}).reset_index()\n",
    "\n",
    "    # for each R/D, select just the route_pattern_id with the most daily trips\n",
    "max_rpid2 = day_rpid2.groupby(by=['route_id','direction_id']).apply(lambda g: g[g['daily_trips'] == g['daily_trips'].max()])[['route_pattern_id','daily_trips']].reset_index()\n",
    "max_rpid2 = max_rpid2[['route_id','direction_id','route_pattern_id']].rename(columns = {'route_pattern_id':'route_pattern_id_new'})\n",
    "\n",
    "# because there are several cases where the max is held by two different route_pattern_ids, chose one arbitrarily\n",
    "max_rpid2 = max_rpid2.drop_duplicates(subset=['route_id','direction_id'])\n",
    "\n",
    "max_rpid2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the route_pattern_ids for just R/H/D combos that have < 3 trips in each of the 4 TOD periods\n",
    "update_table = needs_update.merge(max_rpid2, how='left', on=['route_id','direction_id'])\n",
    "\n",
    "update_table = update_table.drop_duplicates(subset=['route_id','direction_id','trip_headsign'])\n",
    "\n",
    "update_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update those route_pattern_ids!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_update3 = trips_update1.merge(\n",
    "    update_table[['route_id','direction_id','trip_headsign','route_pattern_id_new']], \n",
    "    how='left',on=['route_id','direction_id','trip_headsign'])\n",
    "\n",
    "trips_update3['route_pattern_id'] = np.where(\n",
    "    (\n",
    "        (trips_update3['route_pattern_id_new'].isna())), \n",
    "    trips_update3['route_pattern_id'], \n",
    "    trips_update3['route_pattern_id_new'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update headsign for all trips with updated route_pattern_id\n",
    "rpid_th = trips_update3.groupby(by=['route_pattern_id','trip_headsign','block_id','shape_id','service_id']).agg({'trip_id':'count'}).reset_index()\n",
    "rpid_th_max = rpid_th.groupby(by=['route_pattern_id']).apply(lambda g: g[g['trip_id'] == g['trip_id'].max()]).reset_index(drop=True)\n",
    "rpid_th_max = rpid_th_max.drop_duplicates(subset='route_pattern_id')\n",
    "trips_update4 = trips_update3.merge(\n",
    "    rpid_th_max[['route_pattern_id','trip_headsign','block_id','shape_id','service_id']], \n",
    "    how='left',on=['route_pattern_id'], suffixes=(None, '_to_rplce'))\n",
    "trips_update4['trip_headsign'] = trips_update4['trip_headsign_to_rplce']\n",
    "trips_update4['block_id'] = trips_update4['block_id_to_rplce']\n",
    "#trips_update4['shape_id'] = trips_update4['shape_id_to_rplce']\n",
    "trips_update4['service_id'] = trips_update4['service_id_to_rplce']\n",
    "trips_update4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CHECKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_update3[trips_update3['trip_id'].duplicated(keep=False)]\n",
    "\n",
    "len(trips_update4['trip_id'].unique())\n",
    "len(trips_update4['route_pattern_id'].unique())\n",
    "len(bus_trips['route_pattern_id'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_update5 = trips_update4[gtfsfeeds_dfs.trips.columns].merge(bus_trips[['trip_id','route_pattern_id','trip_headsign']], how='left', on='trip_id', suffixes=(None,'_old'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trips_update5.query('route_pattern_id != route_pattern_id_old').query('trip_headsign == trip_headsign_old'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replace Trips Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_keep_safe = gtfsfeeds_dfs.trips\n",
    "# keep trips just busses for next part too\n",
    "gtfsfeeds_dfs.trips = trips_update5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Generic Stop Times & Stop Sequence per Route Pattern\n",
    "\n",
    "- Can't just change route_pattern_id as TransCAD does not use this field to combine trips into routes. There is no effect on the import.\n",
    "- Working theory is that to consolidate routes one must use the stop_times.txt table as it defines the stop sequence for every trip. Theoretically, this is being used to consolidate trips into routes based on whether they have the same stop sequence.\n",
    "\n",
    "Plan:\n",
    "- Explore if stop times differ depending on TOD or if only realtime GTFS takes into account traffic.\n",
    "    - For every route_pattern_id, get the average trip length (in minutes).\n",
    "- For every route_pattern_id, get the average number of minutes between each pair of stops in the stop sequence.\n",
    "- Replace the stop times and sequence for trips that had their route_pattern_id updated with the generic stop sequence and times per route_pattern_id created in the previous step. \n",
    "    - Keep the start time and work off of that.\n",
    "    - Arrival time will equal departure time given that the difference is usually less than a minute. Will assume difference in time can be included in the minutes to next arrival time for aggregate modeling purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generic_stop_times_sequence(stop_times, trips):\n",
    "    # flag the first stop in every trip\n",
    "    stop_times['first_stop'] =  0\n",
    "    stop_times.loc[stop_times.groupby('trip_id').stop_sequence.idxmin(),'first_stop'] = 1\n",
    "\n",
    "    rpid_stop_dict = {}\n",
    "    # for each route_pattern_id calculate the most common stop pattern\n",
    "    for rpid in tqdm(trips['route_pattern_id'].unique()):\n",
    "        patterns = []\n",
    "        rpid_trips = trips.query('route_pattern_id == @rpid')['trip_id'].to_list()\n",
    "        rpid_stops = stop_times.query('@rpid_trips in trip_id').sort_values('stop_sequence')\n",
    "        \n",
    "        # for every trip, get stop pattern and save into a list of lists\n",
    "        for tid in rpid_stops['trip_id']:\n",
    "            patterns.append([tid,':'.join(rpid_stops.query('trip_id == @tid')['stop_id'].to_list())])\n",
    "        \n",
    "        # make data frame of every trip and its pattern of stops\n",
    "        df = pd.DataFrame(patterns, columns = ['trip_id','stop_pattern'])\n",
    "        # get most common stop_pattern\n",
    "        df_grby = df.groupby('stop_pattern').count().reset_index()\n",
    "        max_stop_pattern = df_grby.query('trip_id == trip_id.max()')['stop_pattern'].to_list()\n",
    "        # get all the trip_ids that represent the route_pattern_id's most popular stop pattern\n",
    "        rep_trips = df.query('stop_pattern == @max_stop_pattern[0]')['trip_id'].to_list()\n",
    "\n",
    "        rpid_stop_dict[rpid] = rep_trips\n",
    "\n",
    "    return(rpid_stop_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpid_stop_dict = generic_stop_times_sequence(gtfsfeeds_dfs.stop_times, gtfsfeeds_dfs.trips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_to_majority_stop_pattern(trips, rpid_stop_dict, stop_times):\n",
    "    # for every row where the route_pattern_id has been updated & doesn't have the majority stop pattern\n",
    "    trip_stop_replace = {}\n",
    "    trip_shape_replace = {}\n",
    "    for idx, row in tqdm(gtfsfeeds_dfs.trips.iterrows()):\n",
    "        # identify trip_id\n",
    "        tid = row['trip_id']\n",
    "        # get the route_pattern_id for the trip\n",
    "        rpid = row['route_pattern_id']\n",
    "\n",
    "        if (tid not in rpid_stop_dict[rpid]): # if trip_id is not in the majority stop pattern\n",
    "            # get all of the trips associated with that route_pattern_id (and stop sequence pattern)\n",
    "            all_trips = rpid_stop_dict[rpid]\n",
    "\n",
    "            # get the start time (for first stop) for trip_id\n",
    "            start_time = stop_times.query('(trip_id == @tid) & (first_stop == 1)')\n",
    "            # get the start time (for first stop) for all trips that share same route_pattern_id\n",
    "            all_start_times = stop_times.query('(trip_id in @all_trips) & (first_stop == 1)')\n",
    "\n",
    "            # create list getting the difference in start times between the selected trip and all the other trips that share the same route_pattern_id\n",
    "            test_list = [[x,(abs(start_time['departure_time_sec']- all_start_times.query('trip_id == @x'))['departure_time_sec'].iloc[0])] for x in all_start_times['trip_id']]\n",
    "            close = {}\n",
    "            close = {sub[0]:sub[1] for sub in test_list}\n",
    "                \n",
    "            # get trip with the minimum difference in start time    \n",
    "            min_t = min(close, key=close.get)\n",
    "            if tid != min_t:\n",
    "                trip_stop_replace[tid] = min_t\n",
    "                trip_shape_replace[tid] = trips.query('trip_id == @min_t')['shape_id'].values[0]\n",
    "        else:\n",
    "            trip_shape_replace[tid] = trips.query('trip_id == @tid')['shape_id'].values[0]\n",
    "    \n",
    "    trip_shape_replace_tab = pd.DataFrame.from_dict(\n",
    "        trip_shape_replace, orient='index').reset_index().rename(\n",
    "            columns = {'index':'trip_id', 0:'shape_id_update'})\n",
    "    \n",
    "    trips['shape_id_new'] = trips['trip_id'].apply(lambda x: trip_shape_replace[x])\n",
    "\n",
    "    trips['shape_id'] = trips['shape_id_new']\n",
    "\n",
    "    # keep only relevant columns\n",
    "    trips_clean = trips.drop(columns=['route_pattern_id_old','trip_headsign_old','shape_id_new'])\n",
    "\n",
    "    return(trips_clean, trip_stop_replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips, trip_stop_replace = update_to_majority_stop_pattern(gtfsfeeds_dfs.trips, rpid_stop_dict, gtfsfeeds_dfs.stop_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_stop_times_new_pattern(stop_times, trips, trip_stop_replace, stop_times_columns):\n",
    "    stop_times = stop_times.sort_values('stop_sequence')\n",
    "    # for every trip (in dictionary with value = trip sharing route_pattern_id with closest start time)\n",
    "    for tid in tqdm(trip_stop_replace.values()):\n",
    "    # update value of time_between_stops for every stop in the selected trip\n",
    "        # time between stops calculated by subtracting the prior departure_time_sec from current (why stop_sequence order is important)\n",
    "        stop_times.loc[stop_times.loc[:,'trip_id'] == str(tid), 'time_between_stops'] = stop_times.loc[stop_times.loc[:,'trip_id'] == str(tid), 'departure_time_sec'].diff()\n",
    "\n",
    "    for trip in tqdm(trip_stop_replace.keys()):\n",
    "        start_time = stop_times.query('(trip_id == @trip) & (first_stop == 1)')['departure_time_sec']\n",
    "        # drop old stop times\n",
    "        stop_times = stop_times.drop(\n",
    "            stop_times.loc[stop_times['trip_id']==trip].index)\n",
    "        # grab new stop times\n",
    "        new_trip = trip_stop_replace[trip]\n",
    "        nst = stop_times.query('trip_id == @new_trip')\n",
    "        nst['trip_id'] = trip\n",
    "\n",
    "        # replace the start time, then calculate the stop times by the departure_time_sec difference\n",
    "        nst.loc[nst.loc[:,'first_stop']==1,'departure_time_sec'] = int(start_time.iloc[0])\n",
    "        nst.loc[nst.loc[:,'first_stop']==1,'time_between_stops'] = int(start_time.iloc[0])\n",
    "        nst['departure_time_sec'] = nst['time_between_stops'].cumsum()\n",
    "\n",
    "        # recalc arrival/dep times\n",
    "        nst['arrival_time'] = pd.to_datetime(nst['departure_time_sec'],unit='s').astype('str').str[11:19]\n",
    "        nst['departure_time'] = nst['arrival_time']\n",
    "\n",
    "        #keep only relevant columns\n",
    "        nst = nst[stop_times.columns]\n",
    "\n",
    "        stop_times = pd.concat([stop_times,nst])\n",
    "    # keep only relevant columns and sort\n",
    "    stop_times = stop_times[stop_times_columns].sort_values(by=['trip_id','stop_sequence'])\n",
    "    return(stop_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_times = update_stop_times_new_pattern(gtfsfeeds_dfs.stop_times, gtfsfeeds_dfs.trips, trip_stop_replace, gtfsfeeds_dfs.stop_times.columns)\n",
    "gtfsfeeds_dfs.stop_times = stop_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge back the filtered out trips\n",
    "gtfsfeeds_dfs.trips = pd.concat([trips, non_bus_trips])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtfsfeeds_dfs.stop_times.to_csv(out_path+r\"/stop_times.txt\")\n",
    "gtfsfeeds_dfs.trips.to_csv(out_path+r\"/trips.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "urbanAccess",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2a2b370b38977169157650f5355ad729af7f449719cdba662b087dc855e43e33"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
