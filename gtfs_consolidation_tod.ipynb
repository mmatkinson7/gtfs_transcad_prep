{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consolidating Route Patterns in GTFS\n",
    "\n",
    "Occurs after running R script that filters the trips, schedules, etc. by a chosen date.\n",
    "This part first came after an import to TransCAD but this is an alternate approach of consolidating solely based on GTFS instead of trying to join the calculated trips per Route_Name back to the route_pattern_id. There isn't a great field to join back on as we don't know exactly how TransCAD is translating the GTFS information into the Route_Names and how the routes are being consolidated as the number of unique route patterns differs between TransCAD and GTFS with GTFS having less unique route patterns in its trip table *(both post R filtering)\n",
    "\n",
    "As discussed with Marty and Sabiheh, the goal of this consolidation step is to re-assign route patterns with low numbers of trips across the day ( less than three in each time period) to route_pattern_ids with the most trips within their matching Route, Direction, and Headsign. This gets filtered down to the trip level where if a trip's route pattern is consolidated, it will be replaced with the route_pattern_id with max trips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('agg')  # allows notebook to be tested in Travis\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandana as pdna\n",
    "import time\n",
    "\n",
    "import urbanaccess as ua\n",
    "from urbanaccess.config import settings\n",
    "from urbanaccess.gtfsfeeds import feeds\n",
    "from urbanaccess import gtfsfeeds\n",
    "from urbanaccess.gtfs.gtfsfeeds_dataframe import gtfsfeeds_dfs\n",
    "from urbanaccess.network import ua_network, load_network\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required bbox including all of Massachusetts and RI as well as parts of NH, CT, NY\n",
    "bbox = (-73.7207, 41.1198, -69.7876, 43.1161)\n",
    "# path to the downloaded and cleaned gtfs - mbta recap file for fall 2018\n",
    "#   this could also be a folder of gtfs folders (pre merge of multiple gtfs)\n",
    "\n",
    "path_to_gtfs = r\"C:\\Users\\matkinson.AD\\Downloads\\Nov12_Sandbox\\Part_2_GTFS_R\\mbta2018_102418_20221109\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking GTFS text file header whitespace... Reading files using encoding: utf-8 set in configuration.\n",
      "GTFS text file header whitespace check completed. Took 0.12 seconds\n",
      "--------------------------------\n",
      "Processing GTFS feed: mbta2018_102418_20221109\n",
      "The unique agency id: mbta was generated using the name of the agency in the agency.txt file.\n",
      "Unique agency id operation complete. Took 0.02 seconds\n",
      "Unique GTFS feed id operation complete. Took 0.00 seconds\n",
      "No GTFS feed stops were found to be outside the bounding box coordinates\n",
      "mbta2018_102418_20221109 GTFS feed stops: coordinates are in northwest hemisphere. Latitude = North (90); Longitude = West (-90).\n",
      "Appended route type to stops\n",
      "Appended route type to stop_times\n",
      "--------------------------------\n",
      "Added descriptive definitions to stops, routes, stop_times, and trips tables\n",
      "Successfully converted ['departure_time'] to seconds past midnight and appended new columns to stop_times. Took 1.04 seconds\n",
      "1 GTFS feed file(s) successfully read as dataframes:\n",
      "     mbta2018_102418_20221109\n",
      "     Took 2.17 seconds\n"
     ]
    }
   ],
   "source": [
    "loaded_feeds = ua.gtfs.load.gtfsfeed_to_df(gtfsfeed_path= path_to_gtfs,\n",
    "                                           validation=True,\n",
    "                                           verbose=True,\n",
    "                                           bbox=bbox,\n",
    "                                           remove_stops_outsidebbox=False,\n",
    "                                           append_definitions=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Needs/Steps:\n",
    "- Number of trips per time period per route_pattern_id\n",
    "    - Midpoint time of each trip\n",
    "    - Each trip classified by TOD (based on midpoint)\n",
    "    - Sum of trips per TOD by route_pattern_id\n",
    "- route_pattern_id with most daily trips per Route, Direction, Headsign\n",
    "    - Sum all tod trips per route_pattern_id\n",
    "    - grab just the max per Route, Direction, Headsign (but keep route_pattern_id)\n",
    "- consolidate route patterns by Route, Direction, Headsign\n",
    "    - if route_pattern_id has less than 3 trips in each of the 4 TODs, replace with max trips route_pattern_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_start_stop_times(stop_times):    \n",
    "    chocula =0 \n",
    "    for trip_id in stop_times['trip_id'].unique():\n",
    "        max_row = stop_times.query('trip_id==@trip_id').query('stop_sequence == stop_sequence.max()')[['trip_id','arrival_time']]\n",
    "        min_row = stop_times.query('trip_id==@trip_id').query('stop_sequence == stop_sequence.min()')[['trip_id','arrival_time']]\n",
    "        r2 = min_row.merge(max_row, how='left', on='trip_id', suffixes = ('_start','_end'))\n",
    "        if chocula == 0:\n",
    "            flintstone = pd.DataFrame(r2)\n",
    "        else:\n",
    "            flintstone=pd.concat([flintstone,r2])\n",
    "        chocula +=1\n",
    "    return(flintstone)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "simpson = get_start_stop_times(gtfsfeeds_dfs.stop_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the results! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simpson['arrival_time_end'].str.split(':').str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simpson.query('arrival_time_end.str.split(\":\").str[0].astype(\"int32\") > 23')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simpson.query('arrival_time_start.str.split(\":\").str[0].astype(\"int32\") < 6').sort_values(by='arrival_time_start')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((simpson['arrival_time_end'].str.split(\":\").str[1]).astype('int32')/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start work again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_tod(start_stop):\n",
    "    start_stop['at_end_dec'] = (\n",
    "        (\n",
    "            (start_stop['arrival_time_end'].str.split(\":\").str[0]).astype('int32')\n",
    "            +\n",
    "            ((start_stop['arrival_time_end'].str.split(\":\").str[1]\n",
    "            ).astype('int32')/60)))\n",
    "    start_stop['at_start_dec'] = (\n",
    "        (\n",
    "            (start_stop['arrival_time_start'].str.split(\":\").str[0]).astype('int32')\n",
    "            +\n",
    "            ((start_stop['arrival_time_start'].str.split(\":\").str[1]\n",
    "            ).astype('int32')/60)))\n",
    "    \n",
    "    start_stop['midpoint'] = start_stop['at_start_dec'] + ((start_stop['at_end_dec']-start_stop['at_start_dec'])/2)\n",
    "    start_stop['tod'] = np.where(start_stop['midpoint'].between(6.50,9.50),'AM', np.where(\n",
    "        start_stop['midpoint'].between(9.50,15.00), 'MD', np.where(\n",
    "            start_stop['midpoint'].between(15.00,19.00),'PM', 'NT' \n",
    "        )\n",
    "            ) \n",
    "        )\n",
    "    \n",
    "    return start_stop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smurf = assign_tod(simpson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smurf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tod_trips = gtfsfeeds_dfs.trips.merge(smurf[['trip_id','tod']], how='left', on='trip_id')\n",
    "tod_rpid = tod_trips.groupby(by=['route_pattern_id','tod']).agg({'tod':'count', 'route_id':'first','trip_headsign':'first','direction_id':'first'})\n",
    "tod_rpid = tod_rpid.rename(columns = {'tod':'trips_per_tod'}).reset_index()\n",
    "tod_rpid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "byrpid = tod_rpid.groupby(\n",
    "    by=['route_id','trip_headsign','direction_id','route_pattern_id']).sum(\n",
    "        'trips_per_tod').reset_index().rename(columns={'trips_per_tod':'daily_trips'})\n",
    "\n",
    "gby = byrpid.set_index(['route_id','direction_id','trip_headsign'])\n",
    "gby.index = ['{}_{}_{}'.format(i, j, k) for i, j, k in gby.index]\n",
    "chocula = 0\n",
    "for name in gby.index:\n",
    "    if len(pd.DataFrame(gby.loc[name,:]).transpose()) > 1:\n",
    "        max_row = pd.DataFrame(gby.loc[name,:]).query('daily_trips == daily_trips.max()')\n",
    "    else:\n",
    "        max_row = pd.DataFrame(gby.loc[name,:]).transpose().query('daily_trips == daily_trips.max()')\n",
    "    if chocula == 0:\n",
    "        flintstone = pd.DataFrame(max_row)\n",
    "    else:\n",
    "        flintstone=pd.concat([flintstone,max_row])\n",
    "    chocula +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "woolf = tod_rpid.pivot_table(index = ['route_pattern_id','route_id','trip_headsign','direction_id'], columns = ['tod'])\n",
    "\n",
    "needs_update = woolf['trips_per_tod'].reset_index().fillna(0).query('AM < 3 & MD < 3 & PM < 3 & NT < 3')\n",
    "needs_update = needs_update.set_index(['route_id','direction_id','trip_headsign'])\n",
    "needs_update.index = ['{}_{}_{}'.format(i, j, k) for i, j, k in needs_update.index]\n",
    "needs_update.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flintstone.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_table = needs_update.reset_index().merge(flintstone.reset_index(), how='left', on='index', suffixes = ('_old','_max')).drop_duplicates('route_pattern_id_old')\n",
    "update_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick break to calculate some numbers per route pattern id before they are updated!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smurf['trip_length'] = smurf['at_end_dec'] - smurf['at_start_dec']\n",
    "smurf['trip_length_time'] = smurf['trip_length'].astype('int32').astype('str') + \":\" + ((smurf['trip_length'] % 1)*60).astype('int32').astype('str') + \":00\"\n",
    "smurf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papa_smurf = smurf.merge(gtfsfeeds_dfs.trips[['trip_id','route_pattern_id']], how='left', on='trip_id')\n",
    "smurfette = papa_smurf.sort_values('midpoint').groupby(by=['route_pattern_id','tod']).agg({'trip_length':['mean','median','count','first','last']})\n",
    "smurfette = smurfette.droplevel(0,axis=1).reset_index()\n",
    "smurfette['first_last_dif'] = round(smurfette['first'] - smurfette['last'],2)\n",
    "smurfette['first_last_dif_min'] = smurfette['first_last_dif']*60\n",
    "smurfette"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Break over - update those route_pattern_ids!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tripstrips = gtfsfeeds_dfs.trips.merge(update_table[['route_pattern_id_old','route_pattern_id_max']], how='left', left_on='route_pattern_id', right_on='route_pattern_id_old')\n",
    "\n",
    "tripstrips['route_pattern_id'] = np.where(tripstrips['route_pattern_id_old'].isna(), tripstrips['route_pattern_id'], tripstrips['route_pattern_id_max'])\n",
    "\n",
    "tripstrips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tripstrips['route_pattern_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gtfsfeeds_dfs.trips['route_pattern_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtfsfeeds_dfs.trips = tripstrips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Generic Stop Times & Stop Sequence per Route Pattern\n",
    "\n",
    "- Can't just change route_pattern_id as TransCAD does not use this field to combine trips into routes. There is no effect on the import.\n",
    "- Working theory is that to consolidate routes one must use the stop_times.txt table as it defines the stop sequence for every trip. Theoretically, this is being used to consolidate trips into routes based on whether they have the same stop sequence.\n",
    "\n",
    "Plan:\n",
    "- Explore if stop times differ depending on TOD or if only realtime GTFS takes into account traffic.\n",
    "    - For every route_pattern_id, get the average trip length (in minutes).\n",
    "- For every route_pattern_id, get the average number of minutes between each pair of stops in the stop sequence.\n",
    "- Replace the stop times and sequence for trips that had their route_pattern_id updated with the generic stop sequence and times per route_pattern_id created in the previous step. \n",
    "    - Keep the start time and work off of that.\n",
    "    - Arrival time will equal departure time given that the difference is usually less than a minute. Will assume difference in time can be included in the minutes to next arrival time for aggregate modeling purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_stop_replace = {}\n",
    "\n",
    "gtfsfeeds_dfs.stop_times['first_stop'] =  0\n",
    "gtfsfeeds_dfs.stop_times.loc[gtfsfeeds_dfs.stop_times.groupby('trip_id').stop_sequence.idxmin(),'first_stop'] = 1\n",
    "\n",
    "for idx, row in gtfsfeeds_dfs.trips.query('(~route_pattern_id_old.isna()) & (route_pattern_id_old != route_pattern_id_max)').iterrows():\n",
    "    tid = row['trip_id']\n",
    "    rpid = gtfsfeeds_dfs.trips.query('trip_id == @tid').route_pattern_id\n",
    "\n",
    "    all_trips = gtfsfeeds_dfs.trips.query('route_pattern_id == @rpid.iloc[0]').trip_id\n",
    "\n",
    "    start_time = gtfsfeeds_dfs.stop_times.query('(trip_id == @tid) & (first_stop == 1)')\n",
    "    all_start_times = gtfsfeeds_dfs.stop_times.query('(trip_id in @all_trips) & (first_stop == 1)')\n",
    "\n",
    "    test_list = [[x,(abs(start_time['departure_time_sec']- all_start_times.query('trip_id == @x'))['departure_time_sec'].iloc[0])] for x in all_start_times['trip_id']]\n",
    "    close = {}\n",
    "    close = {sub[0]:sub[1] for sub in test_list}\n",
    "        \n",
    "    min_t = min(close, key=close.get)\n",
    "    trip_stop_replace[tid] = min_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtfsfeeds_dfs.stop_times = gtfsfeeds_dfs.stop_times.sort_values('stop_sequence')\n",
    "\n",
    "for tid in trip_stop_replace.values():\n",
    "    \n",
    "    gtfsfeeds_dfs.stop_times.loc[gtfsfeeds_dfs.stop_times.loc[:,'trip_id'] == str(tid), 'time_between_stops'] = gtfsfeeds_dfs.stop_times.loc[gtfsfeeds_dfs.stop_times.loc[:,'trip_id'] == str(tid), 'departure_time_sec'].diff()\n",
    "\n",
    "gtfsfeeds_dfs.stop_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtfsfeeds_dfs.stop_times.query('time_between_stops > 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for trip in trip_stop_replace.keys():\n",
    "    start_time = gtfsfeeds_dfs.stop_times.query('(trip_id == @trip) & (first_stop == 1)')['departure_time_sec']\n",
    "    # drop old stop times\n",
    "    gtfsfeeds_dfs.stop_times = gtfsfeeds_dfs.stop_times.drop(\n",
    "        gtfsfeeds_dfs.stop_times.loc[gtfsfeeds_dfs.stop_times['trip_id']==trip].index, inplace=True)\n",
    "    # grab new stop times\n",
    "    nst = gtfsfeeds_dfs.stop_times.query('trip_id == @trip_stop_replace[trip]')\n",
    "    # before updating them, calculate the time between stops\n",
    "    nst.loc[nst.loc[:,'trip_id'] == trip_stop_replace[trip], 'time_between_stops'] = nst.loc[nst.loc[:,'trip_id'] == trip_stop_replace[trip], 'departure_time_sec'].diff()\n",
    "\n",
    "    nst['trip_id'] = trip\n",
    "    # replace the start time, then calculate the stop times by the departure_time_sec difference\n",
    "    nst.loc[nst[:,'first_stop']==1]['departure_time_sec'] = start_time\n",
    "    nst.loc[nst[:,'first_stop']==1]['time_between_stops'] = start_time\n",
    "    nst['departure_time_sec'] = nst['time_between_stops'].cumsum()\n",
    "\n",
    "    # recalc arrival/dep times\n",
    "    nst['arrival_time'] = datetime.timedelta(seconds=nst['departure_time_sec']).astype('str')\n",
    "    nst['departure_time'] = nst['arrival_time']\n",
    "\n",
    "    #keep only relevant columns\n",
    "    nst = nst[gtfsfeeds_dfs.stop_times.columns]\n",
    "\n",
    "    gtfsfeeds_dfs.stop_times = pd.concat([gtfsfeeds_dfs.stop_times,nst])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtfsfeeds_dfs.stop_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tripstrips[gtfsfeeds_dfs.trips.columns].to_csv(r\"C:\\Users\\matkinson.AD\\Downloads\\sandbox_vetday\\trips.txt\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('urbanAccess')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2a2b370b38977169157650f5355ad729af7f449719cdba662b087dc855e43e33"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
