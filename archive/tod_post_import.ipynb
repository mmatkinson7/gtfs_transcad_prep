{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = r\"J:\\Shared drives\\TMD_TSA\\Model\\networks\\Transit\\gtfs\\mbta_2019\\4_imported_gtfs\"\n",
    "gtfs_base = r\"J:\\Shared drives\\TMD_TSA\\Model\\networks\\Transit\\gtfs\\mbta_2019\\3_gtfs_post_move_stops\"\n",
    "\n",
    "base = r\"J:\\Shared drives\\TMD_TSA\\Model\\networks\\Transit\\gtfs\\bnrd\\4_imported_gtfs\"\n",
    "gtfs_base = r\"J:\\Shared drives\\TMD_TSA\\Model\\networks\\Transit\\gtfs\\bnrd\\2_gtfs_py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcad_routes = pd.read_csv(base + r\"\\Transit Routes.csv\", low_memory = False)\n",
    "\n",
    "trips = pd.read_csv(gtfs_base + r\"\\trips.txt\", low_memory = False)\n",
    "stop_times = pd.read_csv(gtfs_base + r\"\\stop_times.txt\", low_memory = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_start_stop_times(stop_times):    \n",
    "    '''for every trip, grab the start time and stop time of the trip\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    stop_times : df\n",
    "        gtfs stop_times.txt in df format\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "        flintstone : df\n",
    "            df with start and stop times per trip\n",
    "\n",
    "    '''\n",
    "    chocula =0 \n",
    "    for trip_id in stop_times['trip_id'].unique():\n",
    "        max_row = stop_times.query('trip_id==@trip_id').query('stop_sequence == stop_sequence.max()')[['trip_id','arrival_time']]\n",
    "        min_row = stop_times.query('trip_id==@trip_id').query('stop_sequence == stop_sequence.min()')[['trip_id','arrival_time']]\n",
    "        r2 = min_row.merge(max_row, how='left', on='trip_id', suffixes = ('_start','_end'))\n",
    "        if chocula == 0:\n",
    "            flintstone = pd.DataFrame(r2)\n",
    "        else:\n",
    "            flintstone=pd.concat([flintstone,r2])\n",
    "        chocula +=1\n",
    "    return(flintstone)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_tod(start_stop):\n",
    "    '''calculate midpoint of trip, use midpoint to assign TOD\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    start_stop : df\n",
    "        df with start and stop times per trip\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    start_stop :\n",
    "        df with start time, stop time, midpoint time, and TOD per trip\n",
    "\n",
    "    '''\n",
    "    \n",
    "    start_stop['at_end_dec'] = (\n",
    "        (\n",
    "            (start_stop['arrival_time_end'].str.split(\":\").str[0]).astype('int32')\n",
    "            +\n",
    "            ((start_stop['arrival_time_end'].str.split(\":\").str[1]\n",
    "            ).astype('int32')/60)))\n",
    "    start_stop['at_start_dec'] = (\n",
    "        (\n",
    "            (start_stop['arrival_time_start'].str.split(\":\").str[0]).astype('int32')\n",
    "            +\n",
    "            ((start_stop['arrival_time_start'].str.split(\":\").str[1]\n",
    "            ).astype('int32')/60)))\n",
    "    \n",
    "    start_stop['midpoint'] = start_stop['at_start_dec'] + ((start_stop['at_end_dec']-start_stop['at_start_dec'])/2)\n",
    "    start_stop['tod'] = np.where(start_stop['midpoint'].between(6.50,9.50),'AM', np.where(\n",
    "        start_stop['midpoint'].between(9.50,15.00), 'MD', np.where(\n",
    "            start_stop['midpoint'].between(15.00,19.00),'PM', 'NT' \n",
    "        )\n",
    "            ) \n",
    "        )\n",
    "    \n",
    "    return start_stop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_stop = get_start_stop_times(stop_times) # simpson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_stop_tod = assign_tod(start_stop) # smurf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fred = [\"Red\",\"Orange\",\"Blue\",\"Green-E\",\"Green-B\",\"Green-C\",\"Green-D\",\"Mattapan\",\"Boat-F4\",\"CR-Worcester\",\"CR-Kingston\"\n",
    ",\"CR-Haverhill\",\"CR-Providence\",\"CR-Newburyport\",\"CR-Needham\",\"CR-Middleborough\",\"CR-Lowell\",\"CR-Greenbush\",\"CR-Franklin\"\n",
    ",\"CR-Fitchburg\",\"CR-Fairmount\",\"Boat-F1\",\"Boat-F2\",\"Boat-F3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "route_name = {}\n",
    "for t in tcad_routes['Trip']:\n",
    "    rn = tcad_routes.query('Trip == @t')['Route_Name'].to_list()[0]\n",
    "    gtfs_n = trips.query('trip_id == @t')['route_pattern_id']\n",
    "    route_name[rn] = gtfs_n.to_list()[0]\n",
    "    #group_trips = trips.query('route_pattern_id == @gtfs_n')['trip_id']\n",
    "    #sst = start_stop_tod.query('trip_id in @group_trips')\n",
    "    #count_tod = sst.groupby(by='tod').count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rpid</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [index]\n",
       "Index: []"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if multiple route patterns per route name\n",
    "reltab = pd.DataFrame.from_dict(route_name, orient='index', columns = ['rpid']).reset_index()\n",
    "reltab.groupby('rpid').count().query('index > 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You are trying to merge on object and float64 columns. If you wish to proceed you should use pd.concat",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mj:\\Shared drives\\TMD_TSA\\Model\\networks\\Transit\\gtfs\\import_scripts\\tod_post_import.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/j%3A/Shared%20drives/TMD_TSA/Model/networks/Transit/gtfs/import_scripts/tod_post_import.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m trips2 \u001b[39m=\u001b[39m trips\u001b[39m.\u001b[39;49mmerge(reltab, how\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mleft\u001b[39;49m\u001b[39m'\u001b[39;49m, left_on \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mroute_pattern_id\u001b[39;49m\u001b[39m'\u001b[39;49m,right_on \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mrpid\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39mrename(columns\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m'\u001b[39m\u001b[39mRoute_Name\u001b[39m\u001b[39m'\u001b[39m})\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\tdm23_env_1\\lib\\site-packages\\pandas\\core\\frame.py:9190\u001b[0m, in \u001b[0;36mDataFrame.merge\u001b[1;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m   9171\u001b[0m \u001b[39m@Substitution\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   9172\u001b[0m \u001b[39m@Appender\u001b[39m(_merge_doc, indents\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m   9173\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmerge\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   9186\u001b[0m     validate: \u001b[39mstr\u001b[39m \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   9187\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame:\n\u001b[0;32m   9188\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mreshape\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmerge\u001b[39;00m \u001b[39mimport\u001b[39;00m merge\n\u001b[1;32m-> 9190\u001b[0m     \u001b[39mreturn\u001b[39;00m merge(\n\u001b[0;32m   9191\u001b[0m         \u001b[39mself\u001b[39;49m,\n\u001b[0;32m   9192\u001b[0m         right,\n\u001b[0;32m   9193\u001b[0m         how\u001b[39m=\u001b[39;49mhow,\n\u001b[0;32m   9194\u001b[0m         on\u001b[39m=\u001b[39;49mon,\n\u001b[0;32m   9195\u001b[0m         left_on\u001b[39m=\u001b[39;49mleft_on,\n\u001b[0;32m   9196\u001b[0m         right_on\u001b[39m=\u001b[39;49mright_on,\n\u001b[0;32m   9197\u001b[0m         left_index\u001b[39m=\u001b[39;49mleft_index,\n\u001b[0;32m   9198\u001b[0m         right_index\u001b[39m=\u001b[39;49mright_index,\n\u001b[0;32m   9199\u001b[0m         sort\u001b[39m=\u001b[39;49msort,\n\u001b[0;32m   9200\u001b[0m         suffixes\u001b[39m=\u001b[39;49msuffixes,\n\u001b[0;32m   9201\u001b[0m         copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m   9202\u001b[0m         indicator\u001b[39m=\u001b[39;49mindicator,\n\u001b[0;32m   9203\u001b[0m         validate\u001b[39m=\u001b[39;49mvalidate,\n\u001b[0;32m   9204\u001b[0m     )\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\tdm23_env_1\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:106\u001b[0m, in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[39m@Substitution\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mleft : DataFrame or named Series\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     90\u001b[0m \u001b[39m@Appender\u001b[39m(_merge_doc, indents\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m     91\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmerge\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m     validate: \u001b[39mstr\u001b[39m \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    105\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame:\n\u001b[1;32m--> 106\u001b[0m     op \u001b[39m=\u001b[39m _MergeOperation(\n\u001b[0;32m    107\u001b[0m         left,\n\u001b[0;32m    108\u001b[0m         right,\n\u001b[0;32m    109\u001b[0m         how\u001b[39m=\u001b[39;49mhow,\n\u001b[0;32m    110\u001b[0m         on\u001b[39m=\u001b[39;49mon,\n\u001b[0;32m    111\u001b[0m         left_on\u001b[39m=\u001b[39;49mleft_on,\n\u001b[0;32m    112\u001b[0m         right_on\u001b[39m=\u001b[39;49mright_on,\n\u001b[0;32m    113\u001b[0m         left_index\u001b[39m=\u001b[39;49mleft_index,\n\u001b[0;32m    114\u001b[0m         right_index\u001b[39m=\u001b[39;49mright_index,\n\u001b[0;32m    115\u001b[0m         sort\u001b[39m=\u001b[39;49msort,\n\u001b[0;32m    116\u001b[0m         suffixes\u001b[39m=\u001b[39;49msuffixes,\n\u001b[0;32m    117\u001b[0m         copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m    118\u001b[0m         indicator\u001b[39m=\u001b[39;49mindicator,\n\u001b[0;32m    119\u001b[0m         validate\u001b[39m=\u001b[39;49mvalidate,\n\u001b[0;32m    120\u001b[0m     )\n\u001b[0;32m    121\u001b[0m     \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39mget_result()\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\tdm23_env_1\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:703\u001b[0m, in \u001b[0;36m_MergeOperation.__init__\u001b[1;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    695\u001b[0m (\n\u001b[0;32m    696\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mleft_join_keys,\n\u001b[0;32m    697\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mright_join_keys,\n\u001b[0;32m    698\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjoin_names,\n\u001b[0;32m    699\u001b[0m ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_merge_keys()\n\u001b[0;32m    701\u001b[0m \u001b[39m# validate the merge keys dtypes. We may need to coerce\u001b[39;00m\n\u001b[0;32m    702\u001b[0m \u001b[39m# to avoid incompatible dtypes\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_coerce_merge_keys()\n\u001b[0;32m    705\u001b[0m \u001b[39m# If argument passed to validate,\u001b[39;00m\n\u001b[0;32m    706\u001b[0m \u001b[39m# check if columns specified as unique\u001b[39;00m\n\u001b[0;32m    707\u001b[0m \u001b[39m# are in fact unique.\u001b[39;00m\n\u001b[0;32m    708\u001b[0m \u001b[39mif\u001b[39;00m validate \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\tdm23_env_1\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:1256\u001b[0m, in \u001b[0;36m_MergeOperation._maybe_coerce_merge_keys\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1250\u001b[0m     \u001b[39m# unless we are merging non-string-like with string-like\u001b[39;00m\n\u001b[0;32m   1251\u001b[0m     \u001b[39melif\u001b[39;00m (\n\u001b[0;32m   1252\u001b[0m         inferred_left \u001b[39min\u001b[39;00m string_types \u001b[39mand\u001b[39;00m inferred_right \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m string_types\n\u001b[0;32m   1253\u001b[0m     ) \u001b[39mor\u001b[39;00m (\n\u001b[0;32m   1254\u001b[0m         inferred_right \u001b[39min\u001b[39;00m string_types \u001b[39mand\u001b[39;00m inferred_left \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m string_types\n\u001b[0;32m   1255\u001b[0m     ):\n\u001b[1;32m-> 1256\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1258\u001b[0m \u001b[39m# datetimelikes must match exactly\u001b[39;00m\n\u001b[0;32m   1259\u001b[0m \u001b[39melif\u001b[39;00m needs_i8_conversion(lk\u001b[39m.\u001b[39mdtype) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m needs_i8_conversion(rk\u001b[39m.\u001b[39mdtype):\n",
      "\u001b[1;31mValueError\u001b[0m: You are trying to merge on object and float64 columns. If you wish to proceed you should use pd.concat"
     ]
    }
   ],
   "source": [
    "trips2 = trips.merge(reltab, how='left', left_on = 'route_pattern_id',right_on = 'rpid').rename(columns={'index':'Route_Name'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trips['route_pattern_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trips.query('route_id not in @fred')['route_pattern_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tcad_routes['Route_Name'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips.query('route_pattern_id not in @reltab.rpid').query('route_id not in @fred')['route_pattern_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trips.query('route_id ==\"34E\"')['route_pattern_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tcad_routes.query('Route == \"34E\"'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips34e = trips.query('route_pattern_id == \"34E-4-0\"')['trip_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_stop_tod.query('trip_id in @trips34e').groupby('tod').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tod_trips = trips2.merge(start_stop_tod).groupby(by=['Route_Name','tod']).agg({'trip_id':'count'}).rename(columns = {'trip_id':'num_trips'}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tod_pivot = tod_trips.pivot(index='Route_Name', columns = 'tod', values = 'num_trips').reset_index()\n",
    "tod_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AM: \"+str(tod_pivot['AM'].sum()))\n",
    "print(\"MD: \"+str(tod_pivot['MD'].sum()))\n",
    "print(\"PM: \"+str(tod_pivot['PM'].sum()))\n",
    "print(\"NT: \"+str(tod_pivot['NT'].sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips2.merge(start_stop_tod).query('route_id not in @fred').groupby('tod').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tod_pivot.to_csv(base+r\"\\tod_pivot.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tdm23_env_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b4cec7a9f7ba5ce94103bf32c92036542d6dedb966a6b7444eec057d4972f9d5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
